{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e612513a",
   "metadata": {},
   "source": [
    "# BUNDESDATA RAG \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b29fce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests\n",
    "from typing import Any, List \n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from ollama._types import ResponseError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35864d7e",
   "metadata": {},
   "source": [
    "## Daten laden und vorbereiten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76fbc9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEURL = \"https://verkehr.autobahn.de/o/autobahn\"\n",
    "SERVICES = [\n",
    "    \"roadworks\",\n",
    "    \"parking_lorry\",\n",
    "    \"warning\",\n",
    "    \"closure\",\n",
    "    \"electric_charging_station\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af51da19",
   "metadata": {},
   "source": [
    "**stringify_all_fields:**\n",
    "Conversion of a python object (dict) into string required for document indexing and processing using langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0ef81c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify_all_fields(obj: Any) -> str:\n",
    "    return json.dumps(obj, ensure_ascii=False, indent=2, sort_keys=True) #ensure ascii=False to handle special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6478c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items(payload: Any, service: str) -> List[Any]:\n",
    "    # if payload is a dict with the service as a key and the value is a list, return that list\n",
    "    if isinstance(payload, dict) and isinstance(payload.get(service), list):\n",
    "        return payload[service]\n",
    "    # if payload is already a list, return it directly\n",
    "    if isinstance(payload, list):\n",
    "        return payload\n",
    "    return [payload]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07476368",
   "metadata": {},
   "source": [
    "**to_documents:**\n",
    "Helper to convert the json response (dict) into langchain documents using the stringify_all_fields() function to get a text representation of all fields.\n",
    "Additionally, we capture data about the road_id, service, title, description to give unique markers to roads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3aa2ebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_documents(road_id: str, service: str, items: List[Any], source_url: str) -> List[Document]:\n",
    "    docs: List[Document] = [] # initialize empty list\n",
    "    for i, item in enumerate(items):\n",
    "        # define title and description\n",
    "        title = \"\"\n",
    "        description = \"\"\n",
    "        if isinstance(item, dict):\n",
    "            title = item.get(\"title\", \"\") or item.get(\"name\", \"\") # if title is empty, try name\n",
    "            description = item.get(\"description\", \"\") or item.get(\"text\", \"\") # if description is empty, try text\n",
    "\n",
    "        page_content = \"\\n\".join(\n",
    "            # structured content\n",
    "            [\n",
    "                f\"road: {road_id}\",\n",
    "                f\"service: {service}\",\n",
    "                (f\"title: {title}\" if title else \"\"),\n",
    "                (f\"description: {description}\" if description else \"\"),\n",
    "                \"\",\n",
    "                \"ALL_FIELDS_JSON:\",\n",
    "                # use stringify_all_fields to convert item to string\n",
    "                stringify_all_fields(item),\n",
    "            ]\n",
    "        ).strip()\n",
    "        # create document and add to list\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=page_content,\n",
    "                metadata={\n",
    "                    \"road_id\": road_id,\n",
    "                    \"service\": service,\n",
    "                    \"source_url\": source_url,\n",
    "                    \"item_index\": i,\n",
    "                    \"title\": title,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b6eb36",
   "metadata": {},
   "source": [
    "**build_documents():**\n",
    "This function does the API calls. We go through all road ids and services (endpoints) available and convert them into a list of langchain documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48657df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_documents() -> List[Document]:\n",
    "    docs: List[Document] = [] # initialize empty list\n",
    "    with requests.Session() as s:\n",
    "        # get list of roads\n",
    "        roads_resp = s.get(BASEURL, timeout=20)\n",
    "        roads_resp.raise_for_status()\n",
    "        roads = roads_resp.json()[\"roads\"]\n",
    "        # iterate over roads and services\n",
    "        for road_id in roads:\n",
    "            for service in SERVICES:\n",
    "                # fetch service data for every road\n",
    "                url = f\"{BASEURL}/{road_id}/services/{service}\"\n",
    "                resp = s.get(url, timeout=20)\n",
    "                resp.raise_for_status()\n",
    "                # get payload\n",
    "                payload = resp.json()\n",
    "                # extract items with helper function\n",
    "                items = get_items(payload, service)\n",
    "                docs.extend(to_documents(road_id, service, items, url)) # convert to documents with helper function\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cd6048",
   "metadata": {},
   "source": [
    "## Embedding Model initialisieren (mit dem Uni-Ollama-Server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6bae58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_URL = \"http://132.199.138.16:11434\"\n",
    "LLM_MODEL = \"gpt-oss:20b\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59700610",
   "metadata": {},
   "source": [
    "**Split text using RecursiveCharacterTextSplitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5bfbdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29121\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\", \",\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs = build_documents()\n",
    "\n",
    "splitted_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "print(len(splitted_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60164fe7",
   "metadata": {},
   "source": [
    "**add_in_batches():**\n",
    "\n",
    "We encountered a EOF status code: 500 Error during the embedding request. To fix this, we batched the documents in even smaller batches (batch_size = 64 here) to avoid the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b954bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_in_batches(\n",
    "    vectorstore: Chroma,\n",
    "    docs: List[Document],\n",
    "    batch_size: int = 64,\n",
    "    max_retries: int = 3,\n",
    "):\n",
    "    \n",
    "    total = len(docs)\n",
    "    for start in range(0, total, batch_size):\n",
    "        batch = docs[start : start + batch_size]\n",
    "\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                vectorstore.add_documents(batch)\n",
    "                break\n",
    "            except ResponseError as e:\n",
    "                if attempt == max_retries:\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b9aee6",
   "metadata": {},
   "source": [
    "**Initialize Ollama-Embedding-Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86285e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs in Chroma: 29121\n"
     ]
    }
   ],
   "source": [
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    base_url=LLM_URL\n",
    ")\n",
    "\n",
    "try:\n",
    "    vectorstore.delete_collection()\n",
    "\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"autobahn-rag\", embedding_function=ollama_embeddings)\n",
    "\n",
    "add_in_batches(\n",
    "    vectorstore,\n",
    "    splitted_docs,\n",
    "    batch_size = 32,\n",
    "    max_retries=3\n",
    ")\n",
    "\n",
    "print(\"Docs in Chroma:\", vectorstore._collection.count())\n",
    "\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "403aed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=LLM_MODEL,\n",
    "    base_url=LLM_URL,\n",
    "    temperature=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37649338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Parkplatz auf der A3\n",
      "\n",
      "[ANSWER]: **Antwort:**\n",
      "\n",
      "Im bereitgestellten Kontext sind keine konkreten Angaben zu Parkpl√§tzen auf der A3 vorhanden. Die vorliegenden Informationen beschreiben lediglich Bauarbeiten und Verkehre auf der A3 (z.‚ÄØB. Umbau am AK Kaiserberg, A3‚ÄØ|‚ÄØK√∂ln‚ÄëOst ‚Äì K√∂nigsforst, A3‚ÄØ|‚ÄØArnheim). \n",
      "\n",
      "**Was im Kontext fehlt:**\n",
      "\n",
      "- Keine Erw√§hnung von Parkpl√§tzen, Rastpl√§tzen oder Tankstellen, die speziell auf der A3 liegen.\n",
      "- Keine Angaben zu Baustellen, die Parkfl√§chen schaffen oder einschr√§nken k√∂nnten.\n",
      "- Kein Hinweis auf besondere Parkm√∂glichkeiten f√ºr Baustellenbesucher oder Anwohner.\n",
      "\n",
      "**Allgemeine Information zu Parkpl√§tzen auf der A3:**\n",
      "\n",
      "- Auf deutschen Autobahnen (inkl. der A3) ist dauerhaftes Parken grunds√§tzlich nicht erlaubt.  \n",
      "- Nur an offiziellen Rast- und Tankstellen gibt es zeitlich begrenzte Parkpl√§tze, die in der Regel f√ºr kurze Aufenthalte (z.‚ÄØB. 30‚ÄØ‚Äì‚ÄØ60‚ÄØMinuten) gedacht sind.  \n",
      "- F√ºr Baustellenbesucher k√∂nnen spezielle Arbeits- oder Anwohnerparkpl√§tze eingerichtet werden, diese liegen aber meist nicht direkt auf dem Autobahnprofil, sondern in angrenzenden Bereichen (z.‚ÄØB. an Servicewegen oder an den Baustellenr√§ndern).  \n",
      "- Bei gr√∂√üeren Bauprojekten wie dem ‚ÄûUmbau AK Kaiserberg‚Äú oder den anderen in deinem Kontext genannten Bauarbeiten kann es vorkommen, dass tempor√§re Parkfl√§chen f√ºr Arbeitskr√§fte oder Materiallieferanten geschaffen werden. Diese sind jedoch meistens in der N√§he der Baustelle und nicht mitten auf der Autobahn selbst.\n",
      "\n",
      "Falls du einen Parkplatz f√ºr eine konkrete Baustelle oder ein bestimmtes Ereignis auf der A3 suchst, w√§re es sinnvoll, die jeweilige Baustellenverwaltung oder die Verkehrsbeh√∂rde zu kontaktieren, um genaue Angaben zu bekommen.\n"
     ]
    }
   ],
   "source": [
    "question = \"Parkplatz auf der A3\"\n",
    "context_docs = vector_retriever.invoke(question)\n",
    "context = \"\\n\\n---\\n\\n\".join(d.page_content for d in context_docs)\n",
    "\n",
    "prompt = (\"Beantworte die Frage mit Kontext.\"\n",
    "          \"Wenn Informationen fehlen, sage, was im Kontext nicht enthalten ist, aber antworte trotzdem so weit wie m√∂glich.\\n\\n\"\n",
    "          f\"Frage: {question} \\n\\n Kontext:{context}\"\n",
    ")\n",
    "\n",
    "answer = llm.invoke(prompt)\n",
    "print(\"\\nQuestion:\", question)\n",
    "print(\"\\n[ANSWER]:\", answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53bf200",
   "metadata": {},
   "source": [
    "## Gradio UI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbfe1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def rag_answer(question: str):\n",
    "    # retrieve relevant docs \n",
    "    context_docs = vector_retriever.invoke(question)\n",
    "\n",
    "    # docs to string\n",
    "    context = \"\\n\\n---\\n\\n\".join(d.page_content for d in context_docs)\n",
    "\n",
    "    # promt for LLM to generate answer based on context\n",
    "    prompt = (\n",
    "        \"Beantworte die Frage mit Kontext. \"\n",
    "        \"Wenn Informationen fehlen, sage, was im Kontext nicht enthalten ist, aber antworte trotzdem so weit wie m√∂glich.\\n\\n\"\n",
    "        f\"Frage: {question}\\n\\n\"\n",
    "        f\"Kontext:\\n{context}\"\n",
    "    )\n",
    "\n",
    "    # LLM call\n",
    "    answer = llm.invoke(prompt)\n",
    "\n",
    "    return answer.content\n",
    "\n",
    "\n",
    "with gr.Blocks(title=\"Bundesdata RAG\") as demo:\n",
    "    gr.Markdown(\"# üöó Autobahn Bundesdata RAG\")\n",
    "    # instruction for user\n",
    "    gr.Markdown(\n",
    "        \"Stelle eine Frage zum Thema deutsche Autobahnen. \"\n",
    "        \"Es wird Kontext via Retriever geholt und das LLM antwortet soweit m√∂glich aus dem Kontext. \"\n",
    "        \"Im Kontext vorhandene Themenbereiche sind roadworks, parkinglorries, warnings, closures und electric charging stations.\"\n",
    "    )\n",
    "\n",
    "    question_in = gr.Textbox(\n",
    "        label=\"Stelle deine Frage\",\n",
    "        placeholder=\"z.B. gib mir alle Warnings auf der A93\",\n",
    "        lines=2,\n",
    "    )\n",
    "\n",
    "    ask_btn = gr.Button(\"Antwort generieren\")\n",
    "\n",
    "    answer_out = gr.Textbox(\n",
    "        label=\"Antwort\",\n",
    "        lines=10,\n",
    "        interactive=False\n",
    "    )\n",
    "\n",
    "   \n",
    "    # button click to generate answer\n",
    "    ask_btn.click(\n",
    "        fn=rag_answer,\n",
    "        inputs=question_in,\n",
    "        outputs=[answer_out],\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bundesdata-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
